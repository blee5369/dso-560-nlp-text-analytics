{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amber-scoop",
   "metadata": {},
   "source": [
    "# Homework 4 (Due Friday, Nov. 19th, 11:59pm PST)\n",
    "\n",
    "1. Identify **three pairs of documents** in the McDonalds review dataset that have over 0.85 cosine similarity using average token word2vec embeddings from spacy.\n",
    "\n",
    "Lets load dependencies, our data, and inspect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mysterious-deposit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:12:32.985626Z",
     "start_time": "2021-11-20T02:12:25.658829Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.jpijnswnnan3ce6lli5fwsphut2vxmth.gfortran-win_amd64.dll\n",
      "C:\\Users\\drpow\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "mcd = pd.read_csv('mcdonalds-yelp-negative-reviews.csv', encoding=\"ISO-8859-1\")\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "japanese-liver",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:12:32.998454Z",
     "start_time": "2021-11-20T02:12:32.986434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>city</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>679455653</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>I'm not a huge mcds lover, but I've been to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>679455654</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Terrible customer service. I came in at 9:30pm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>679455655</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>First they \"lost\" my order, actually they gave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>679455656</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>I see I'm not the only one giving 1 star. Only...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>679455657</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Well, it's McDonald's, so you know what the fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id     city                                             review\n",
       "0  679455653  Atlanta  I'm not a huge mcds lover, but I've been to be...\n",
       "1  679455654  Atlanta  Terrible customer service. I came in at 9:30pm...\n",
       "2  679455655  Atlanta  First they \"lost\" my order, actually they gave...\n",
       "3  679455656  Atlanta  I see I'm not the only one giving 1 star. Only...\n",
       "4  679455657  Atlanta  Well, it's McDonald's, so you know what the fo..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-pitch",
   "metadata": {},
   "source": [
    "## Cleaning data \n",
    "\n",
    "While spacy handles tokenization, POS, stopword, and lemmatiziation steps of data cleaning we can still benefit from consolidating concepts using the logic below to map menu items and common themes back to single concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "combined-control",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:12:33.010454Z",
     "start_time": "2021-11-20T02:12:33.000455Z"
    }
   },
   "outputs": [],
   "source": [
    "def consolidate_concepts(text):\n",
    "    cleaned_reviews = []\n",
    "    for review in text['review']:\n",
    "        review = re.sub(r\"(?:mcdonald's?|mcdonalds?|macdonalds?|mcds?)\",'_MCDONALD_', review, flags=re.IGNORECASE)\n",
    "        review = re.sub(r\"(?:burgers?|cheeseburgers?|hamburgers?|hamburgersandwiches?)\",'_HAMBURGER_', review, flags=re.IGNORECASE)\n",
    "        review = re.sub(r\"(?:McNuggets?|nuggets?|nugs?)\",'_NUGGET_', review, flags=re.IGNORECASE)\n",
    "        review = re.sub(r\"(?:fries?|frys?|french fries?)\",'_FRIES_', review, flags=re.IGNORECASE)\n",
    "        cleaned_reviews.append(review)\n",
    "    \n",
    "    text['review_cleaned'] = cleaned_reviews\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "separate-generic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:12:33.117455Z",
     "start_time": "2021-11-20T02:12:33.012457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>city</th>\n",
       "      <th>review</th>\n",
       "      <th>review_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>679455653</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>I'm not a huge mcds lover, but I've been to be...</td>\n",
       "      <td>I'm not a huge _MCDONALD_ lover, but I've been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>679455654</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Terrible customer service. I came in at 9:30pm...</td>\n",
       "      <td>Terrible customer service. I came in at 9:30pm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>679455655</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>First they \"lost\" my order, actually they gave...</td>\n",
       "      <td>First they \"lost\" my order, actually they gave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>679455656</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>I see I'm not the only one giving 1 star. Only...</td>\n",
       "      <td>I see I'm not the only one giving 1 star. Only...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>679455657</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>Well, it's McDonald's, so you know what the fo...</td>\n",
       "      <td>Well, it's _MCDONALD_, so you know what the fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id     city                                             review  \\\n",
       "0  679455653  Atlanta  I'm not a huge mcds lover, but I've been to be...   \n",
       "1  679455654  Atlanta  Terrible customer service. I came in at 9:30pm...   \n",
       "2  679455655  Atlanta  First they \"lost\" my order, actually they gave...   \n",
       "3  679455656  Atlanta  I see I'm not the only one giving 1 star. Only...   \n",
       "4  679455657  Atlanta  Well, it's McDonald's, so you know what the fo...   \n",
       "\n",
       "                                      review_cleaned  \n",
       "0  I'm not a huge _MCDONALD_ lover, but I've been...  \n",
       "1  Terrible customer service. I came in at 9:30pm...  \n",
       "2  First they \"lost\" my order, actually they gave...  \n",
       "3  I see I'm not the only one giving 1 star. Only...  \n",
       "4  Well, it's _MCDONALD_, so you know what the fo...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_mcd = consolidate_concepts(mcd)\n",
    "cleaned_mcd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-toner",
   "metadata": {},
   "source": [
    "## Run reviews through spaCy pipeline\n",
    "    \n",
    "Lets make the data simple for analysis by creating columns for each spacy NLP attribute we want.\n",
    "\n",
    "While running the pipeline lets compare similarity of document embeddings to each document that has already been processed, this means we will have pairwise comparisons bewteen all documents. we will write reviews with above .85 cos similarity out to be further analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d880f2be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:12:33.124457Z",
     "start_time": "2021-11-20T02:12:33.118455Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_spaCy_cols(text):\n",
    "    similar_reviews=pd.DataFrame()\n",
    "    review_1 = []\n",
    "    review_2 = []\n",
    "    similarity_l = []\n",
    "    for n in tqdm(range(len(text['review_cleaned']))):\n",
    "        target_raw_review=text.iloc[n].review\n",
    "        target_review=text.iloc[n].review_cleaned\n",
    "        nlp_target_review=nlp(target_review)\n",
    "        clean_review_list=list(text['review_cleaned'])\n",
    "        for object_review in text.iloc[n+1:].review_cleaned:\n",
    "            object_idx = clean_review_list.index(object_review)\n",
    "            object_raw_review=text.iloc[object_idx].review\n",
    "            nlp_object_review = nlp(object_review)\n",
    "            similarity = nlp_target_review.similarity(nlp_object_review)\n",
    "            if similarity > 0.85:\n",
    "                review_1.append(target_raw_review)\n",
    "                review_2.append(object_raw_review)\n",
    "                similarity_l.append(similarity)\n",
    "            if len(review_1) > 2: #Going through all rows takes too much time, so we limited max 4 matches per review\n",
    "                break\n",
    "    similar_reviews['review_1']=review_1\n",
    "    similar_reviews['review_2']=review_2\n",
    "    similar_reviews['similarity']=similarity_l\n",
    "    return similar_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "addressed-deployment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:15:15.242725Z",
     "start_time": "2021-11-20T02:14:16.494534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e6413cdf494258adffcc803dbde4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1525 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similar_reviews = add_spaCy_cols(mcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-imperial",
   "metadata": {},
   "source": [
    "## Lets get 3 random reviews that were over .85 similarity\n",
    "\n",
    "Because the reviews are all related to negative experiences at mcdonalds many reviews have high similarity, if these reviews were mixed with other sentiment reviews, or reviews from other establishments this code would be more effective at generating insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b392306",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:15:18.589240Z",
     "start_time": "2021-11-20T02:15:18.582246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Review 1-------------\n",
      "Y'all want $15 an hour but I'm waiting 30 minutes for mcnuggets. Y'all got me McHeated.\n",
      "-----------Review 2-------------\n",
      "Can't expect much from a McDonald's but this is one of the worst I've been to. The employees are rude, they always get your orders wrong and then have an attitude when they have to fix it. Freshness is an issue as well. Wait time in the drive thru isn't too bad, but don't make the mistake of seeing \n",
      "SIMILARITY:  0.85966\n",
      "\n",
      "-----------Review 1-------------\n",
      "I work directly across from this place and go for coffee usually - rarely the breakfast. They ALWAYS get my order wrong - especially through the drive-through. Be careful for overcharging - and be specific with your order, otherwise they will give you a larger size than what you asked for. It has ha\n",
      "-----------Review 2-------------\n",
      "3 stars is about as good a review as a McDonald's can get. It's McDonald's, what more can you say? I reviewed this one because their coca-cola had the best syrup to soda ratio I have ever had, it tasted like straight out of the bottle. Kinda random review, I guess.\n",
      "SIMILARITY:  0.94905\n",
      "\n",
      "-----------Review 1-------------\n",
      "Its McDonald's so we already know the food is decent and slightly over priced. But the staff here is bad. None of them speak english. They are slow. They will mess your order up so be sure to check it before leaving the drive thru.\n",
      "-----------Review 2-------------\n",
      "Worst mcd's ever. Looks fancy but management obviously doesn't care about service. My sister wanted some fries and a cheeseburger one night after playing some pool at Rack Daddy's so I headed over to this location because it's a block away from home. After paying them $5 for something I'm sure cost \n",
      "SIMILARITY:  0.96942\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, row in similar_reviews.sample(3).iterrows():\n",
    "    print(\"-----------Review 1-------------\", row['review_1'][:300], sep ='\\n')\n",
    "    print(\"-----------Review 2-------------\", row['review_2'][:300], sep ='\\n')\n",
    "    print(\"SIMILARITY: \", round(row['similarity'], 5))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-initial",
   "metadata": {},
   "source": [
    "# Using the `SMS_test` and `SMS_train` datasets, build a classification model \n",
    "\n",
    "(you can simply use the `sklearn.linear_model.LogisticRegression` model used. Please attempt at least two of the vectorization techniques below:\n",
    "* `CountVectorization`\n",
    "* `TfIdfVectorization`\n",
    "* `word2vec` spacy document-level vectors\n",
    "\n",
    "Make sure you perform the following:\n",
    "* use train/test split\n",
    "* use proper model evaluation metrics\n",
    "* text preprocessing (regex, stemming/lemmatization, stopword removal, grouping entities, etc.)\n",
    "\n",
    "A discussion of the following:\n",
    "* **What techniques** you tried to improve the performance of your model.\n",
    "* What you would try to do, given more time, that would improve the performance of your model.\n",
    "* Provide an example of two **error cases** - a false positive and a false negative - that your model got wrong, and why the model did not predict the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "limiting-blind",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:16:50.747538Z",
     "start_time": "2021-11-20T02:16:50.732797Z"
    }
   },
   "outputs": [],
   "source": [
    "sms_test = pd.read_csv('SMS_test.csv',  encoding=\"ISO-8859-1\")\n",
    "sms_train = pd.read_csv('SMS_train.csv',  encoding=\"ISO-8859-1\")\n",
    "sms_train.head(5)\n",
    "\n",
    "\n",
    "# map our spam ham values to binary\n",
    "sms_train['Label'] = sms_train['Label'].map({'Spam':1, 'Non-Spam':0})\n",
    "sms_test['Label'] = sms_test['Label'].map({'Spam':1, 'Non-Spam':0})\n",
    "\n",
    "# seperate our y labels for future training\n",
    "y_train = sms_train['Label']\n",
    "y_test = sms_test['Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "described-bahrain",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:16:51.356432Z",
     "start_time": "2021-11-20T02:16:51.350408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our test data is ~12% of the total available data\n"
     ]
    }
   ],
   "source": [
    "print(f\"Our test data is ~{round((sms_test.shape[0] / (sms_train.shape[0]+sms_test.shape[0]))*100)}\\\n",
    "% of the total available data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-peninsula",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Functions for Model Evaluation: (Grid search, CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "discrete-republican",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:16:53.403354Z",
     "start_time": "2021-11-20T02:16:53.396121Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## stole this code from another class\n",
    "\n",
    "import itertools as it\n",
    "\n",
    "def clf_score(clf, x_train, y_train, cv=20, n_jobs=-1):\n",
    "    labels = []\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    score = cross_validate(clf, x_train, y_train, scoring='f1', cv=cv, n_jobs=n_jobs,\n",
    "                           return_train_score=True, return_estimator=True)\n",
    "    train_scores.append(score['train_score'])\n",
    "    test_scores.append(score['test_score'])\n",
    "    print(f\"Mean Train Score:{np.mean(score['train_score'])} \\n Mean Test Score:{np.mean(score['test_score'])}\")\n",
    "    return(np.mean(score['test_score']))\n",
    "\n",
    "def get_paramsList(params_grid):\n",
    "    \"\"\"\n",
    "    Create all possible combinations of params.\n",
    "    Returns a list of all param names and a list of all param combinations.\n",
    "    \"\"\"\n",
    "    allNames = sorted(params_grid)\n",
    "    combinations = it.product(*(params_grid[Name] for Name in allNames))\n",
    "    all_params = list(combinations)\n",
    "    return allNames, all_params\n",
    "\n",
    "def param_search(model, X, y, param_grid, verbose = True, scoring = 'ks', \n",
    "                 smote = True, stacking = False, models = None):\n",
    "    \"\"\"\n",
    "    Brute force search through param_grid to find the optimal parameter combination based on the specified score type. \n",
    "    Can be used to search parameters for both stacking model and regular models.\n",
    "    When stacking is True, SMOTE is disabled.\n",
    "    scoring = ['accuracy', 'auc', 'ks']\n",
    "    \"\"\"\n",
    "    param_names, all_params = get_paramsList(param_grid)\n",
    "    print(\"Total combination:\", len(all_params))\n",
    "    best_score = 0\n",
    "    best_param = None\n",
    "    best_smote = None\n",
    "    best_scores = None\n",
    "    \n",
    "    count = 0\n",
    "    labels = []\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    param_list = []\n",
    "    score_list = []\n",
    "    for cur_params in all_params:\n",
    "        params = dict(zip(param_names, cur_params))\n",
    "        model.set_params(**params)\n",
    "        r2 = clf_score(model, X, y, 20, n_jobs=-1)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\t\", params, f\"R2: {r2:.3f}\")\n",
    "\n",
    "        score=r2\n",
    "        \n",
    "        param_list.append(params)\n",
    "        score_list.append(score)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_param = params\n",
    "            best_smote = smote\n",
    "            best_scores = [r2]\n",
    "\n",
    "        count += 1\n",
    "        if count%10 == 0:\n",
    "            print(f\"{count} combinations searched\")\n",
    "    all_scores = pd.DataFrame([param_list,score_list])\n",
    "    print(\"Best param:\", best_param)\n",
    "    print(\"Best scores (f1):\", best_scores)\n",
    "    return best_param, best_scores, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "great-enlargement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:16:53.847244Z",
     "start_time": "2021-11-20T02:16:53.835275Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def find_unique_characters(regex, lines):\n",
    "    \"\"\"\n",
    "    Finds unique characters from a list of strings, almost certainly inefficiently \n",
    "    \n",
    "    \"\"\"\n",
    "    #Match anything that is non alpha-numeric or whitespace, creates list of lists of matching characters\n",
    "    potential_malforms = [re.findall(regex, review) for review in lines]\n",
    "\n",
    "    #lets whittle down this list of lists to a unqiue list, btw this took me way longer than it needed to\n",
    "    unique_malforms = set([char for review in potential_malforms for char in review])\n",
    "    \n",
    "    print(F\"Number of unique potential Malformed Characters: {len(unique_malforms)}, \\n\\nCandidates: {unique_malforms}\")\n",
    "    return unique_malforms\n",
    "\n",
    "def clean_sms(df):\n",
    "    cleaned_message = []\n",
    "    for message in df['Message_body']:\n",
    "        cleaned_message.append(re.sub(r\"[^A-Za-z0-9 ]\",'',message))\n",
    "    \n",
    "    df['cleaned_messages'] = cleaned_message\n",
    "    return df \n",
    "\n",
    "def sms_spaCy_cols(text):\n",
    "    word_embeddings = []\n",
    "    ents = []\n",
    "    ent_type = []\n",
    "    for review in text['Message_body']:\n",
    "        doc = nlp(review)\n",
    "        word_embeddings.append(doc.vector)\n",
    "        ents.append(doc.ents)\n",
    "        for token in doc:\n",
    "            ent_list = []\n",
    "            ent_list.append(token.ent_type_)      \n",
    "        ent_type.append(ent_list)\n",
    "    text[\"doc_embeddings\"] = word_embeddings\n",
    "    text[\"entities\"] = ents\n",
    "    text['entity_types'] = ent_type\n",
    "        \n",
    "    return text\n",
    "\n",
    "#part of speech logic stolen from: https://www.programiz.com/python-programming/methods/set/update\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def tfidf_process_documents(tot_reviews):\n",
    "    \"\"\" For each document, clean punctuation, tag with POS, lemmatize each word and remove stopwords\"\"\"\n",
    "    cleaned_sms = []\n",
    "    for review in tot_reviews:\n",
    "        # Clean punctuation\n",
    "        clean_review = re.sub(r\"[^A-Za-z0-9 ]\",'', review)\n",
    "\n",
    "        # Tokenize into words \n",
    "        lemmatized_word = []\n",
    "        #Tag words with part of speech \n",
    "        nltk_tagged = nltk.pos_tag(nltk.word_tokenize(clean_review))  \n",
    "        wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # lemetize, use part of speech if available\n",
    "        for word, tag in wordnet_tagged:\n",
    "            if tag is None:\n",
    "                #if there is no available tag, append the token as is\n",
    "                lemmatized_word.append(word)\n",
    "            else:        \n",
    "                #else use the tag to lemmatize the token\n",
    "                lemmatized_word.append(lemmatizer.lemmatize(word, tag))\n",
    "\n",
    "        words_clean = []\n",
    "        for word in lemmatized_word:\n",
    "            if word in nltk_stopwords:\n",
    "                continue\n",
    "            words_clean.append(word)\n",
    "        cleaned_review = \" \".join(words_clean)\n",
    "        cleaned_sms.append((cleaned_review))\n",
    "\n",
    "    return cleaned_sms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-minutes",
   "metadata": {},
   "source": [
    "## Word to Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-policy",
   "metadata": {},
   "source": [
    "### Feature engineering (one hot encoding entity types)\n",
    " It may be a variable of interest to have entity types including in aiding the detection of spam, to this end we have created one hot encodings of the entity types mentioned in each sms message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "induced-straight",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:17:02.750580Z",
     "start_time": "2021-11-20T02:16:55.419947Z"
    }
   },
   "outputs": [],
   "source": [
    "# clean sms\n",
    "sms_train = clean_sms(sms_train)\n",
    "sms_test = clean_sms(sms_test)\n",
    "\n",
    "# run through spaCy pipeline\n",
    "sms_train_spacy = sms_spaCy_cols(sms_train)\n",
    "sms_test_spacy = sms_spaCy_cols(sms_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "promising-bullet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:17:03.032440Z",
     "start_time": "2021-11-20T02:17:03.024449Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stole list one hot encoding code from: https://stackoverflow.com/questions/52189126\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "test_onehot = pd.DataFrame(mlb.fit_transform(sms_test_spacy['entity_types']),\n",
    "                   columns=mlb.classes_,\n",
    "                   index=sms_test_spacy['entity_types'].index)\n",
    "\n",
    "train_onehot = pd.DataFrame(mlb.fit_transform(sms_train_spacy['entity_types']),\n",
    "                   columns=mlb.classes_,\n",
    "                   index=sms_train_spacy['entity_types'].index)\n",
    "\n",
    "# drop junk column\n",
    "train_onehot.drop(columns =[''], inplace=True)\n",
    "test_onehot.drop(columns =[''], inplace=True)\n",
    "\n",
    "# join back to main dfs \n",
    "sms_train_spacy = train_onehot.join(sms_train_spacy)\n",
    "sms_test_spacy = test_onehot.join(sms_test_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-welding",
   "metadata": {},
   "source": [
    "### Train test preprocessing and vector flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "appointed-entity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:17:03.413610Z",
     "start_time": "2021-11-20T02:17:03.295035Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pd.concat([pd.DataFrame(sms_train_spacy['doc_embeddings'].values.flat),sms_train_spacy.iloc[:,:8]], axis = 1)\n",
    "X_test = pd.concat([pd.DataFrame(sms_test_spacy['doc_embeddings'].values.flat),sms_test_spacy.iloc[:,:8]], axis = 1)\n",
    "\n",
    "#lets join all our train test data for CV\n",
    "X_trntst = pd.concat([X_train, X_test], ignore_index=True).fillna(0).drop(columns = ['S. No.'])\n",
    "Y_trntst = pd.concat([y_train, y_test], ignore_index=True).fillna(0).drop(columns = ['S. No.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "excess-arrest",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:19:30.366991Z",
     "start_time": "2021-11-20T02:17:03.801612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combination: 4\n",
      "Mean Train Score:1.0 \n",
      " Mean Test Score:0.9108965372432862\n",
      "\t {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': 300, 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_iter': 200, 'solver': 'sgd'} R2: 0.911\n",
      "Mean Train Score:1.0 \n",
      " Mean Test Score:0.9048032533865081\n",
      "\t {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': 100, 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_iter': 200, 'solver': 'sgd'} R2: 0.905\n",
      "Mean Train Score:1.0 \n",
      " Mean Test Score:0.897537113729064\n",
      "\t {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 300, 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_iter': 200, 'solver': 'sgd'} R2: 0.898\n",
      "Mean Train Score:1.0 \n",
      " Mean Test Score:0.8996799708719212\n",
      "\t {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 100, 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_iter': 200, 'solver': 'sgd'} R2: 0.900\n",
      "Best param: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': 300, 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_iter': 200, 'solver': 'sgd'}\n",
      "Best scores (f1): [0.9108965372432862]\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [300, 100],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['sgd'],\n",
    "    'learning_rate':['adaptive'],\n",
    "    'max_iter':[200],\n",
    "    'learning_rate_init':[0.1],\n",
    "    'alpha': [0.0001]\n",
    "}\n",
    "\n",
    "\n",
    "model = MLPClassifier()\n",
    "params, scores, dataframe = param_search(model, X_trntst, Y_trntst, param_grid, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-explorer",
   "metadata": {},
   "source": [
    "## TFIDF Classifier\n",
    "\n",
    "Decent Classifier, mid 70's f1 score, still worse than word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24487459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:19:30.768321Z",
     "start_time": "2021-11-20T02:19:30.763187Z"
    }
   },
   "outputs": [],
   "source": [
    "def tfidf(df):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(2, 2),\n",
    "                                 max_features=500)\n",
    "    X = vectorizer.fit_transform(df['Message_body'])\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    tf_idf = pd.DataFrame(X.toarray(), columns=terms)\n",
    "\n",
    "    return tf_idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "honest-rebate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:19:32.658380Z",
     "start_time": "2021-11-20T02:19:31.145457Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning with regex, POS tagging, lemmatization, stopword removal \n",
    "\n",
    "nltk_stopwords=list(set(stopwords.words('english')))\n",
    "\n",
    "tf_sms_train = tfidf_process_documents(sms_train['Message_body'])\n",
    "tf_sms_test = tfidf_process_documents(sms_test['Message_body'])\n",
    "\n",
    "# TFIDF vectorize\n",
    "tf_X_train = tfidf(sms_train)\n",
    "tf_X_test = tfidf(sms_test)\n",
    "\n",
    "tf_X_trntst = pd.concat([tf_X_train, tf_X_test], ignore_index=True).fillna(0)\n",
    "tf_Y_trntst = pd.concat([y_train, y_test], ignore_index=True).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "improved-interference",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:26:39.039058Z",
     "start_time": "2021-11-20T02:19:33.240659Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combination: 4\n",
      "Mean Train Score:0.9525317270239011 \n",
      " Mean Test Score:0.7263279522335249\n",
      "\t {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': 300, 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_iter': 200, 'solver': 'sgd'} R2: 0.726\n",
      "Mean Train Score:0.9520854966581428 \n",
      " Mean Test Score:0.7453664799253035\n",
      "\t {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': 100, 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_iter': 200, 'solver': 'sgd'} R2: 0.745\n",
      "Mean Train Score:0.9524412601004186 \n",
      " Mean Test Score:0.7286340237849525\n",
      "\t {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 300, 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_iter': 200, 'solver': 'sgd'} R2: 0.729\n",
      "Mean Train Score:0.9525196214589249 \n",
      " Mean Test Score:0.740422870902747\n",
      "\t {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': 100, 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_iter': 200, 'solver': 'sgd'} R2: 0.740\n",
      "Best param: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': 100, 'learning_rate': 'adaptive', 'learning_rate_init': 0.1, 'max_iter': 200, 'solver': 'sgd'}\n",
      "Best scores (f1): [0.7453664799253035]\n",
      "Wall time: 7min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [300, 100],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['sgd'],\n",
    "    'learning_rate':['adaptive'],\n",
    "    'max_iter':[200],\n",
    "    'learning_rate_init':[0.1],\n",
    "    'alpha': [0.0001]\n",
    "}\n",
    "\n",
    "\n",
    "model = MLPClassifier()\n",
    "params, scores, dataframe = param_search(model, tf_X_trntst, tf_Y_trntst, param_grid, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-spank",
   "metadata": {},
   "source": [
    "### Rebuild model to find incorrectly labeled messages \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "chemical-perth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:26:44.899134Z",
     "start_time": "2021-11-20T02:26:40.592747Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drpow\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S. No.</th>\n",
       "      <th>Message_body</th>\n",
       "      <th>Label</th>\n",
       "      <th>cleaned_messages</th>\n",
       "      <th>doc_embeddings</th>\n",
       "      <th>entities</th>\n",
       "      <th>entity_types</th>\n",
       "      <th>tfidf_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>UpgrdCentre Orange customer, you may now claim...</td>\n",
       "      <td>1</td>\n",
       "      <td>UpgrdCentre Orange customer you may now claim ...</td>\n",
       "      <td>[-0.061032206, 0.19988048, -0.07019498, -0.023...</td>\n",
       "      <td>((UpgrdCentre, Orange), (0207, 153, 9153), (26...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Loan for any purpose £500 - £75,000. Homeowner...</td>\n",
       "      <td>1</td>\n",
       "      <td>Loan for any purpose 500  75000 Homeowners  Te...</td>\n",
       "      <td>[-0.11778805, 0.2355668, -0.1783199, -0.033316...</td>\n",
       "      <td>()</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Congrats! Nokia 3650 video camera phone is you...</td>\n",
       "      <td>1</td>\n",
       "      <td>Congrats Nokia 3650 video camera phone is your...</td>\n",
       "      <td>[-0.16864648, 0.17816488, 0.012670864, -0.0110...</td>\n",
       "      <td>((Nokia), (3650), (16, +), (300603), (BCM4284,...</td>\n",
       "      <td>[PERSON]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>URGENT! Your Mobile number has been awarded wi...</td>\n",
       "      <td>1</td>\n",
       "      <td>URGENT Your Mobile number has been awarded wit...</td>\n",
       "      <td>[-0.10755558, 0.18265584, -0.024203632, -0.094...</td>\n",
       "      <td>((2000), (09058094455), (3030), (12hrs))</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Someone has contacted our dating service and e...</td>\n",
       "      <td>1</td>\n",
       "      <td>Someone has contacted our dating service and e...</td>\n",
       "      <td>[-0.012037433, 0.21381874, -0.17289515, 0.0333...</td>\n",
       "      <td>((09111032124),)</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Congratulations ur awarded 500 of CD vouchers ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Congratulations ur awarded 500 of CD vouchers ...</td>\n",
       "      <td>[-0.18412688, 0.0780921, -0.108404435, 0.00680...</td>\n",
       "      <td>((500), (125gift), (&amp;, Free), (2, 100), (87066))</td>\n",
       "      <td>[CARDINAL]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>101</td>\n",
       "      <td>Not directly behind... Abt 4 rows behind ü...</td>\n",
       "      <td>0</td>\n",
       "      <td>Not directly behind Abt 4 rows behind</td>\n",
       "      <td>[0.03910618, 0.051001094, -0.2445561, -0.08481...</td>\n",
       "      <td>((4),)</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>103</td>\n",
       "      <td>URGENT! This is the 2nd attempt to contact U!U...</td>\n",
       "      <td>1</td>\n",
       "      <td>URGENT This is the 2nd attempt to contact UU h...</td>\n",
       "      <td>[-0.0497304, 0.16805999, -0.019893367, -0.0411...</td>\n",
       "      <td>((U!U), (£, 1000CALL), (09071512432), (max£7),...</td>\n",
       "      <td>[CARDINAL]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>106</td>\n",
       "      <td>Wanna have a laugh? Try CHIT-CHAT on your mobi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Wanna have a laugh Try CHITCHAT on your mobile...</td>\n",
       "      <td>[-0.03521142, 0.12336205, -0.15226537, 0.05641...</td>\n",
       "      <td>((4217), (London), (16))</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>110</td>\n",
       "      <td>Should i buy him a blackberry bold 2 or torch....</td>\n",
       "      <td>0</td>\n",
       "      <td>Should i buy him a blackberry bold 2 or torch ...</td>\n",
       "      <td>[-0.050810847, 0.2072405, -0.21790484, -0.1294...</td>\n",
       "      <td>((2),)</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     S. No.                                       Message_body  Label  \\\n",
       "0         1  UpgrdCentre Orange customer, you may now claim...      1   \n",
       "1         2  Loan for any purpose £500 - £75,000. Homeowner...      1   \n",
       "2         3  Congrats! Nokia 3650 video camera phone is you...      1   \n",
       "3         4  URGENT! Your Mobile number has been awarded wi...      1   \n",
       "4         5  Someone has contacted our dating service and e...      1   \n",
       "..      ...                                                ...    ...   \n",
       "99      100  Congratulations ur awarded 500 of CD vouchers ...      1   \n",
       "100     101      Not directly behind... Abt 4 rows behind ü...      0   \n",
       "102     103  URGENT! This is the 2nd attempt to contact U!U...      1   \n",
       "105     106  Wanna have a laugh? Try CHIT-CHAT on your mobi...      1   \n",
       "109     110  Should i buy him a blackberry bold 2 or torch....      0   \n",
       "\n",
       "                                      cleaned_messages  \\\n",
       "0    UpgrdCentre Orange customer you may now claim ...   \n",
       "1    Loan for any purpose 500  75000 Homeowners  Te...   \n",
       "2    Congrats Nokia 3650 video camera phone is your...   \n",
       "3    URGENT Your Mobile number has been awarded wit...   \n",
       "4    Someone has contacted our dating service and e...   \n",
       "..                                                 ...   \n",
       "99   Congratulations ur awarded 500 of CD vouchers ...   \n",
       "100             Not directly behind Abt 4 rows behind    \n",
       "102  URGENT This is the 2nd attempt to contact UU h...   \n",
       "105  Wanna have a laugh Try CHITCHAT on your mobile...   \n",
       "109  Should i buy him a blackberry bold 2 or torch ...   \n",
       "\n",
       "                                        doc_embeddings  \\\n",
       "0    [-0.061032206, 0.19988048, -0.07019498, -0.023...   \n",
       "1    [-0.11778805, 0.2355668, -0.1783199, -0.033316...   \n",
       "2    [-0.16864648, 0.17816488, 0.012670864, -0.0110...   \n",
       "3    [-0.10755558, 0.18265584, -0.024203632, -0.094...   \n",
       "4    [-0.012037433, 0.21381874, -0.17289515, 0.0333...   \n",
       "..                                                 ...   \n",
       "99   [-0.18412688, 0.0780921, -0.108404435, 0.00680...   \n",
       "100  [0.03910618, 0.051001094, -0.2445561, -0.08481...   \n",
       "102  [-0.0497304, 0.16805999, -0.019893367, -0.0411...   \n",
       "105  [-0.03521142, 0.12336205, -0.15226537, 0.05641...   \n",
       "109  [-0.050810847, 0.2072405, -0.21790484, -0.1294...   \n",
       "\n",
       "                                              entities entity_types  \\\n",
       "0    ((UpgrdCentre, Orange), (0207, 153, 9153), (26...           []   \n",
       "1                                                   ()           []   \n",
       "2    ((Nokia), (3650), (16, +), (300603), (BCM4284,...     [PERSON]   \n",
       "3             ((2000), (09058094455), (3030), (12hrs))           []   \n",
       "4                                     ((09111032124),)           []   \n",
       "..                                                 ...          ...   \n",
       "99    ((500), (125gift), (&, Free), (2, 100), (87066))   [CARDINAL]   \n",
       "100                                             ((4),)           []   \n",
       "102  ((U!U), (£, 1000CALL), (09071512432), (max£7),...   [CARDINAL]   \n",
       "105                           ((4217), (London), (16))           []   \n",
       "109                                             ((2),)           []   \n",
       "\n",
       "     tfidf_pred  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "..          ...  \n",
       "99            0  \n",
       "100           1  \n",
       "102           0  \n",
       "105           0  \n",
       "109           1  \n",
       "\n",
       "[70 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPClassifier(activation= 'relu', alpha = 0.0001, hidden_layer_sizes = 100,\\\n",
    "                      learning_rate='adaptive', learning_rate_init= 0.1, max_iter= 200, solver= 'sgd')\n",
    "model.fit(tf_X_train, y_train)\n",
    "\n",
    "\n",
    "# Lets find some incorrectly labeled messages\n",
    "\n",
    "predictions = model.predict(tf_X_test)\n",
    "\n",
    "sms_test['tfidf_pred'] = predictions\n",
    "\n",
    "sms_test[sms_test['tfidf_pred'] != sms_test['Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "assured-warner",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:26:46.132329Z",
     "start_time": "2021-11-20T02:26:46.125329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Should i buy him a blackberry bold 2 or torch. Should i buy him new or used. Let me know. Plus are you saying i should buy the  &lt;#&gt; g wifi ipad. And what are you saying about the about the  &lt;#&gt; g?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_test.iloc[109]['Message_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "unavailable-border",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T02:26:47.418881Z",
     "start_time": "2021-11-20T02:26:47.412880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Someone has contacted our dating service and entered your phone because they fancy you! To find out who it is call from a landline 09111032124 . PoBox12n146tf150p'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_test.iloc[4]['Message_body']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-chester",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Best Test F1 Score:**\n",
    "\n",
    "TFIDF: 0.745\n",
    "\n",
    "Word 2 Vec: \n",
    "0.911\n",
    "\n",
    "--------------------------------------------------------------------------\n",
    "--------------------------------------------------------------------------    \n",
    "\n",
    "* *What techniques* you tried to improve the performance of your model.\n",
    "    - Param grid search \n",
    "    - 20-fold CV to ensure accurate score reporting\n",
    "    - Turning named entity types into one hot encoding categorical variables (word2vec)\n",
    "\n",
    "\n",
    "* What you would try to do, given more time, that would improve the performance of your model.\n",
    "    - More model frameworks (logistic regression, random forest, boosted tress, Light gradient boosting)\n",
    "    - Tinker with params in grid search\n",
    "    - tinker with cv to get different train data splits\n",
    "    - more feature engineering \n",
    "\n",
    "\n",
    "* Provide an example of two **error cases** - a false positive and a false negative - that your model got wrong, and why the model did not predict the correct answer.\n",
    "\n",
    "**Below mis-classifications are from TFIDF:**\n",
    "\n",
    "    - False Positive Original Message (S. No. 110): \n",
    "    \n",
    "<i>\n",
    "Should i buy him a blackberry bold 2 or torch. Should i buy him new or used. Let me know. Plus are you saying i should buy the  &lt;#&gt; g wifi ipad. And what are you saying about the about the  &lt;#&gt; g?'</i>\n",
    "\n",
    "\n",
    "    \n",
    "We can Infer that the model has learned the offers of electronics (blackberry, ipad) are often spam, which we can see is frequent for spam messages in the raw data, and is also why this non-spam discussion of electronics gifts got classified as spam.\n",
    "\n",
    "--------------------------------------------------------------------------    \n",
    "    - False Negative Original Message (S. No. 5)\n",
    "    \n",
    "<i>\n",
    "'Someone has contacted our dating service and entered your phone because they fancy you! To find out who it is call from a landline 09111032124 . PoBox12n146tf150p'\n",
    "</i>\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "False negative is harder to explain as this one as read by a human is very clearly spam, we can hypothesize that there may not be enough mentions of dating services in our spam dataset, our model did not learn this relationship, this could likely be avoided in the future by having a larger dataset with more of these common dating spam messages.\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-serbia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
